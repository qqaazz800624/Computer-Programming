[1] R. L. Siegel, K. D. Miller, and A. Jemal, "Cancer statistics, 2016," CA: A Cancer Journal for Clinicians, vol. 66, no. 1, pp. 7-30, 2016, doi: https://doi.org/10.3322/caac.21332.
[2] S. B. Salem, S. Z. Ali, A. J. Leo, Z. Lachiri, and M. Mkandawire, "Early breast cancer detection and differentiation tool based on tissue impedance characteristics and machine learning," Front Artif Intell, vol. 6, p. 1248977, 2023, doi: 10.3389/frai.2023.1248977.
[3] T. Y. Wu and S. L. Chen, "Breast Cancer Screening Practices and Related Health Beliefs among Taiwanese Nurses," Asia Pac J Oncol Nurs, vol. 4, no. 2, pp. 104-111, Apr-Jun 2017, doi: 10.4103/2347-5625.204495.
[4] D. Thigpen, A. Kappler, and R. Brem, "The Role of Ultrasound in Screening Dense Breasts-A Review of the Literature and Practical Solutions for Implementation," Diagnostics (Basel), vol. 8, no. 1, Mar 16 2018, doi: 10.3390/diagnostics8010020.
[5] M. F. Hou et al., "Comparison of breast mammography, sonography and physical examination for screening women at high risk of breast cancer in taiwan," Ultrasound Med Biol, vol. 28, no. 4, pp. 415-20, Apr 2002, doi: 10.1016/s0301-5629(02)00483-0.
[6] M. E. Danko, K. M. Bennett, J. Zhai, J. R. Marks, and J. A. Olson, Jr., "Improved staging in node-positive breast cancer patients using lymph node ratio: results in 1,788 patients with long-term follow-up," J Am Coll Surg, vol. 210, no. 5, pp. 797-805 e1, 805-7, May 2010, doi: 10.1016/j.jamcollsurg.2010.02.045.
[7] A. Neri et al., "Prognostic value of extracapsular extension of axillary lymph node metastases in T1 to T3 breast cancer," Ann Surg Oncol, vol. 12, no. 3, pp. 246-53, Mar 2005, doi: 10.1245/ASO.2005.02.029.
[8] D. Unal, A. Oguz, and A. Tasdemir, "Rate of metastasis in examined lymph nodes as a predictor of extracapsular extension in patients with axillary node-positive breast cancer," J Nippon Med Sch, vol. 81, no. 6, pp. 372-7, 2014, doi: 10.1272/jnms.81.372.
[9] W. K. Moon, I. L. Chen, A. Yi, M. S. Bae, S. U. Shin, and R. F. Chang, "Computer-aided prediction model for axillary lymph node metastasis in breast cancer using tumor morphological and textural features on ultrasound," Comput Methods Programs Biomed, vol. 162, pp. 129-137, Aug 2018, doi: 10.1016/j.cmpb.2018.05.011.
[10] Y. W. Lee, C. S. Huang, C. C. Shih, and R. F. Chang, "Axillary lymph node metastasis status prediction of early-stage breast cancer using convolutional neural networks," Comput Biol Med, vol. 130, p. 104206, Mar 2021, doi: 10.1016/j.compbiomed.2020.104206.
[11] S. C. Van Alsten et al., "Differences in 21-Gene and PAM50 Recurrence Scores in Younger and Black Women With Breast Cancer," JCO Precis Oncol, vol. 8, p. e2400137, Jul 2024, doi: 10.1200/po.24.00137.
[12] M. L. Giger, "Computer-aided detection and diagnosis/radiomics/machine learning/deep learning in medical imaging," Med Phys, vol. 50 Suppl 1, pp. 50-53, Jun 2023, doi: 10.1002/mp.16025.
[13] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," Nature, vol. 521, no. 7553, pp. 436-44, May 28 2015, doi: 10.1038/nature14539.
[14] T. C. Chiang, Y. S. Huang, R. T. Chen, C. S. Huang, and R. F. Chang, "Tumor Detection in Automated Breast Ultrasound Using 3-D CNN and Prioritized Candidate Aggregation," IEEE Trans Med Imaging, vol. 38, no. 1, pp. 240-249, Jan 2019, doi: 10.1109/TMI.2018.2860257.
[15] X. Li, M. Jia, M. T. Islam, L. Yu, and L. Xing, "Self-Supervised Feature Learning via Exploiting Multi-Modal Data for Retinal Disease Diagnosis," IEEE Trans Med Imaging, vol. 39, no. 12, pp. 4023-4033, Dec 2020, doi: 10.1109/TMI.2020.3008871.
[16] I. Oksuz et al., "Deep Learning-Based Detection and Correction of Cardiac MR Motion Artefacts During Reconstruction for High-Quality Segmentation," IEEE Trans Med Imaging, vol. 39, no. 12, pp. 4001-4010, Dec 2020, doi: 10.1109/TMI.2020.3008930.
[17] M. Chung et al., "Automatic Registration Between Dental Cone-Beam CT and Scanned Surface via Deep Pose Regression Neural Networks and Clustered Similarities," IEEE Trans Med Imaging, vol. 39, no. 12, pp. 3900-3909, Dec 2020, doi: 10.1109/TMI.2020.3007520.
[18] I. J. Goodfellow et al., "Generative Adversarial Nets," Adv Neur In, vol. 27, 2014. [Online]. Available: <Go to ISI>://WOS:000452647101094.
[19] H. C. Shin et al., "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning," IEEE Trans Med Imaging, vol. 35, no. 5, pp. 1285-98, May 2016, doi: 10.1109/TMI.2016.2528162.
[20] Y. F. Li and D. M. Liang, "Safe semi-supervised learning: a brief introduction" Front Comput Sci-Chi, vol. 13, no. 4, pp. 669-676, Aug 2019, doi: 10.1007/s11704-019-8452-2.
[21] A. Jamaludin, T. Kadir, and A. Zisserman, "Self-supervised Learning for Spinal MRIs," Cham, 2017: Springer International Publishing, in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 294-302. 
[22] M. M. Bejani and M. Ghatee, "Convolutional Neural Network With Adaptive Regularization to Classify Driving Styles on Smartphones," Ieee T Intell Transp, vol. 21, no. 2, pp. 543-552, Feb 2020, doi: 10.1109/Tits.2019.2896672.
[23] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler, "Efficient Object Localization Using Convolutional Networks," Proc Cvpr Ieee, pp. 648-656, 2015. [Online]. Available: <Go to ISI>://WOS:000387959200071.
[24] W. X. Zhao et al., "A Survey of Large Language Models," arXiv:2303.18223, March 01, 2023, doi: 10.48550/arXiv.2303.18223.
[25] M. Awais et al., "Foundational Models Defining a New Era in Vision: A Survey and Outlook," arXiv:2307.13721, July 01, 2023, doi: 10.48550/arXiv.2307.13721.
[26] J. Jiao et al., "USFM: A universal ultrasound foundation model generalized to tasks and organs towards label efficient image analysis," Medical Image Analysis, vol. 96, p. 103202, 2024/08/01/ 2024, doi: https://doi.org/10.1016/j.media.2024.103202.
[27] A. Radford et al., "Learning Transferable Visual Models From Natural Language Supervision," presented at the Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, 2021. [Online]. Available: https://proceedings.mlr.press/v139/radford21a.html.
[28] J. M. Chang et al., "Imaging Protocol and Criteria for Evaluation of Axillary Lymph Nodes in the NAUTILUS Trial," J Breast Cancer, vol. 24, no. 6, pp. 554-560, Dec 2021, doi: 10.4048/jbc.2021.24.e47.
[29] J. M. Chang et al., "Imaging Protocol and Criteria for Evaluation of Axillary Lymph Nodes in the NAUTILUS Trial," J Breast Cancer, vol. 24, no. 6, pp. 554-560, Dec 2021, doi: 10.4048/jbc.2021.24.e47.
[30] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall, "Activation Functions: Comparison of trends in Practice and Research for Deep Learning," CoRR, vol. abs/1811.03378, 2018. [Online]. Available: http://arxiv.org/abs/1811.03378.
[31] Y. Zhang and Q. Yang, "An overview of multi-task learning," National Science Review, vol. 5, no. 1, pp. 30-43, 2017, doi: 10.1093/nsr/nwx105.
[32] Y. Zhao, X. Wang, T. Che, G. Bao, and S. Li, "Multi-task deep learning for medical image computing and analysis: A review," Computers in Biology and Medicine, vol. 153, p. 106496, 2023/02/01/ 2023, doi: https://doi.org/10.1016/j.compbiomed.2022.106496.
[33] J. Chowdary, P. Yogarajah, P. Chaurasia, and V. Guruviah, "A Multi-Task Learning Framework for Automated Segmentation and Classification of Breast Tumors From Ultrasound Images," Ultrason Imaging, vol. 44, no. 1, pp. 3-12, Jan 2022, doi: 10.1177/01617346221075769.
[34] Z. Fan et al., "Joint localization and classification of breast masses on ultrasound images using an auxiliary attention-based framework," Med Image Anal, vol. 90, p. 102960, Dec 2023, doi: 10.1016/j.media.2023.102960.
[35] Y. Zhou et al., "Multi-task learning for segmentation and classification of tumors in 3D automated breast ultrasound images," Med Image Anal, vol. 70, p. 101918, May 2021, doi: 10.1016/j.media.2020.101918.
[36] A. Khan et al., "A Survey of the Self Supervised Learning Mechanisms for Vision Transformers," arXiv:2408.17059, August 01, 2024, doi: 10.48550/arXiv.2408.17059.
[37] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum Contrast for Unsupervised Visual Representation Learning," in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13-19 June 2020 2020, pp. 9726-9735, doi: 10.1109/CVPR42600.2020.00975. 
[38] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum Contrast for Unsupervised Visual Representation Learning," in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13-19 June 2020 2020, pp. 9726-9735, doi: 10.1109/CVPR42600.2020.00975. 
[39] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum Contrast for Unsupervised Visual Representation Learning," in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13-19 June 2020 2020, pp. 9726-9735, doi: 10.1109/CVPR42600.2020.00975. 
[40] X. Chen, H. Fan, R. Girshick, and K. J. a. p. a. He, "Improved baselines with momentum contrastive learning," Med Phys, 2020.
[41] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, "Masked Autoencoders Are Scalable Vision Learners," in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 18-24 June 2022 2022, pp. 15979-15988, doi: 10.1109/CVPR52688.2022.01553. 
[42] J. Ma et al., "Segment Anything in Medical Images and Videos: Benchmark and Deployment," arXiv:2408.03322, August 01, 2024, doi: 10.48550/arXiv.2408.03322.
[43] Z. Wang, Z. Wu, D. Agarwal, and J. Sun, "MedCLIP: Contrastive Learning from Unpaired Medical Images and Text," arXiv:2210.10163, October 01, 2022, doi: 10.48550/arXiv.2210.10163.
[44] J. Lee et al., "BioBERT: a pre-trained biomedical language representation model for biomedical text mining," Bioinformatics, vol. 36, pp. 1234-1240, February 01, 2020 2020, doi: 10.1093/bioinformatics/btz682.
[45] C. Li et al., "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day," arXiv:2306.00890, June 01, 2023, doi: 10.48550/arXiv.2306.00890.
[46] M. Badawi, M. Abushanab, S. Bhat, and A. Maier, "Review of Zero-Shot and Few-Shot AI Algorithms in The Medical Domain," arXiv:2406.16143, June 01, 2024, doi: 10.48550/arXiv.2406.16143.
[47] V. Lialin, V. Deshpande, X. Yao, and A. Rumshisky, "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning," arXiv:2303.15647, March 01, 2023, doi: 10.48550/arXiv.2303.15647.
[48] E. J. Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models," arXiv:2106.09685, June 01, 2021, doi: 10.48550/arXiv.2106.09685.
[49] K. Carolan, L. Fennelly, and A. F. Smeaton, "A Review of Multi-Modal Large Language and Vision Models," arXiv:2404.01322, March 01, 2024, doi: 10.48550/arXiv.2404.01322.
[50] J. Wang et al., "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks," arXiv:2408.01319, August 01, 2024, doi: 10.48550/arXiv.2408.01319.
[51] J. Zhang, J. Huang, S. Jin, and S. Lu, "Vision-Language Models for Vision Tasks: A Survey," arXiv:2304.00685, April 01, 2023, doi: 10.48550/arXiv.2304.00685.
[52] J. Li, D. Li, S. Savarese, and S. Hoi, "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models," presented at the Proceedings of the 40th International Conference on Machine Learning, Honolulu, Hawaii, USA, 2023.
[53] J.-B. Alayrac et al., "Flamingo: a Visual Language Model for Few-Shot Learning," arXiv:2204.14198, April 01, 2022, doi: 10.48550/arXiv.2204.14198.
[54] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models," arXiv:2304.10592, April 01, 2023, doi: 10.48550/arXiv.2304.10592.
[55] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models," arXiv:2304.10592, April 01, 2023, doi: 10.48550/arXiv.2304.10592.
[56] W. Dai et al., "InstructBLIP: towards general-purpose vision-language models with instruction tuning," presented at the Proceedings of the 37th International Conference on Neural Information Processing Systems , articleno = 2142 , numpages = 18, 2024.
[57] W. Dai et al., "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning," arXiv:2305.06500, May 01, 2023, doi: 10.48550/arXiv.2305.06500.
[58] W. Wang et al., "CogVLM: Visual Expert for Pretrained Language Models," arXiv:2311.03079, November 01, 2023, doi: 10.48550/arXiv.2311.03079.
[59] J. Zhang, J. Huang, S. Jin, and S. Lu, "Vision-Language Models for Vision Tasks: A Survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 8, pp. 5625-5644, 2024, doi: 10.1109/TPAMI.2024.3369699.
[60] W. Cai, J. Jiang, F. Wang, J. Tang, S. Kim, and J. Huang, "A Survey on Mixture of Experts," arXiv:2407.06204, June 01, 2024, doi: 10.48550/arXiv.2407.06204.
[61] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, "Adaptive Mixtures of Local Experts," Neural Computation, vol. 3, no. 1, pp. 79-87, 1991, doi: 10.1162/neco.1991.3.1.79.
[62] W. Fedus, B. Zoph, and N. Shazeer, "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity," arXiv:2101.03961, January 01, 2021, doi: 10.48550/arXiv.2101.03961.
[63] K. Teja Chitty-Venkata et al., "LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators," arXiv:2411.00136, October 01, 2024, doi: 10.48550/arXiv.2411.00136.
[64] J. Li, D. Li, S. Savarese, and S. Hoi, "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models," arXiv:2301.12597, January 01, 2023, doi: 10.48550/arXiv.2301.12597.
[65] Z. Zhang et al., "Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning," in ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 14-19 April 2024 2024, pp. 1731-1735, doi: 10.1109/ICASSP48485.2024.10446878. 
[66] J. Silva-Rodríguez, H. Chakor, R. Kobbi, J. Dolz, and I. Ben Ayed, "A Foundation Language-Image Model of the Retina (FLAIR): encoding expert knowledge in text supervision," Medical Image Analysis, vol. 99, p. 103357, 2025/01/01/ 2025, doi: https://doi.org/10.1016/j.media.2024.103357.